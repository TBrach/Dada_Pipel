---
title: "LibrarySizeNormalization"
author: "16S class"
date: "January 17, 2017"
output: html_document
---
# Required packages

```{r, message = FALSE}
library(phyloseq); packageVersion("phyloseq")
library(DESeq2); packageVersion("DESeq2")
library(knitr); packageVersion("knitr")
library(ggplot2); packageVersion("ggplot2")
library(dplyr); packageVersion("dplyr")
library(gridExtra); packageVersion("gridExtra")

```

# To DO

- Illustrate the Ratio Distributions from which DESeq decides on the correction factors
- Compare abundances: DESeq, Wilcoxon, is there more
- implement TbyTRatio method
- can you do beta-diversity measures with counts or do you need relative abundances?

# Links


# Questions

- how well can DESeq deal with sparsity?
    - Does assumption of "most taxa are of similar abundance in all samples" hold?
    - is it better to do the normalisation only on more abundant taxa so?
    

# Background

- see slides: 170116\_16S\_Normalisation.pdf
- I currently see two options:
    - do the DESeq normalization but probably only on most abundant taxa that are present in many samples to fulfill the DESeq assumption that most taxa do not "really" change
- work with relative abundances and

# Load seqtab and taxonomy data and source functions

```{r, message = FALSE, warning = FALSE}

datapath <- "/Users/jvb740/MarieCurie_Work/BackgroundKnowledge/16S_Learning/DanFunD_243_maxEE-1"
load(file.path(datapath, "Dada_Data/DenoisedData.RData"))

load(file.path(datapath, "TaxAssign/TaxonomicData.RData"))

functionpath <- "/Users/jvb740/MarieCurie_Work/BackgroundKnowledge/16S_Learning/Dada_Pipel"

source(file.path(functionpath, "Dada_TaxonomyFunctions.R"))
source(file.path(functionpath, "Dada_PlotFunctions.R"))

## At this point you should have seqtab and taxa.species
# NB: I had saved as taxa.80.species, in the current version you have directly taxa.species so the following 2 lines will not be necessary:
taxa.species <- taxa.80.species
rm(taxa.50, taxa.80, taxa.80.species, taxa.50.species, bimFs, bimRs, mergers, mergers.nochim)

```

# Look at Amplicon Distribution, NB most sequences/taxa are only present in very few samples

```{r, message = FALSE, warning = FALSE}

TrList <- AmpliconDistribution(seqtab, PCentage = 30)

TrList[[1]]
```

- **For DESeq normalisation sparsity will be a problem (geometric mean), so I would only use taxa present in many samples, e.g. here 30 % of samples**

```{r, message = FALSE, warning = FALSE}
TrList[[4]]
```

- this will base the normalisation on roughly 70% of the reads

# Short Exploratory Analysis of the taxonomic assignment (taxa.species)

## Determine for each taxonomic level the percentage of sequences that could not be attributed unambiguously with the used minBoot


```{r, message = FALSE, warning = FALSE}
# Number and percentage of assigned taxonomic levels for all amplicons
NAPerC <- NAPerCForTaxLevel(taxa.species)

knitr::kable(NAPerC)

```

# Generate the phyloseq object

```{r, message = FALSE, warning = FALSE}
# Mock sample data
SampleNames <- rownames(seqtab)
set.seed(123)
Age <- sample(x = 20:70, size = length(SampleNames), replace = TRUE)
samdf <- data.frame(Sample = SampleNames, Age = Age)
samdf$Above50 <- Age > 50
rownames(samdf) <- SampleNames


# Generate the phyloseq file
ps <- phyloseq(otu_table(seqtab, taxa_are_rows=FALSE), tax_table(taxa.species), sample_data(samdf))
ps

```

# Normalization a la DESeq

## Filter taxa: keep only taxa present in at least least 30% of samples

```{r, warning = FALSE, message = FALSE}

# Filter Taxa
formals(filttaxa_by_penetrance)
# psf <- filter_taxa(ps, function(x){sum(x != 0) > .3*length(x)}, prune = TRUE) 
FiltList <- filttaxa_by_penetrance(ps, penetrance = 30)
FiltList1 <- filttaxa_by_penetrance(ps, penetrance = 30, MaxCountCheck = T)

# check that the 146 is correct:
# sum(colSums(seqtab != 0) > .3*nrow(seqtab))
f_seqtab <- seqtab[,colSums(seqtab != 0) > .3*nrow(seqtab)]

# Look how sparsity changed
FiltList[["Sparsity"]]
FiltList1[[2]]

# Note how many reads some of the smaples lost
FiltList[["PlotReadsRemoved"]]
FiltList1[[3]]

FiltList[["HistReadsRemoved"]]
FiltList1[[4]]

FiltList[["PlotSampleSumsBeforeAfter"]]
FiltList1[[5]]

# Continue with filtered phyloseq object
psf <- FiltList[[1]]
```

- So this leaves only 146 out of the 3600 taxa, but remember it is still 70 % of the total reads

## Implement own DESeq normalization  

```{r, warning = FALSE, message = FALSE}
# ----  Playing for understanding some basics first ---------
FiltList[["Sparsity"]]
#SparsityPC <- 100*(sum(otu_table(psf) == 0)/(ntaxa(psf)*nsamples(psf))) 
#SparsityPC # still almost 43 %

# ==== Understanding gm_own ======
# NB the prod(x/gm(x)) = 1 only applies when no 0 present, e.g.
prodgm <- function(x){
        GMzeroTrue <- gm_own(x,zeros.count = TRUE)
        GMzeroFALSE <- gm_own(x,zeros.count = FALSE)
        xT <- x/GMzeroTrue
        xF <- x/GMzeroFALSE
        list(xT= xT, xF= xF, GMzeroTrue =  GMzeroTrue,  GMzeroFALSE =  GMzeroFALSE, ProdT = prod(xT[xT>0], na.rm = TRUE),
             ProdF = prod(xF[xF>0], na.rm = TRUE))
}
x <- 1:10
prodgm(x)
y <- x
y[c(1,5)] <- 0
prodgm(y)
g <- y
g[5] <- NA
prodgm(g)
# ======

# ---- USE adjust_LS ------
# formals(adjust_LS) # understand zeros.count, I recommend just default values
# body(adjust_LS)
# environment(adjust_LS)

ListOAdj <- adjust_LS(psf)
# ==== look at some outputs ==== 
ListOAdj[["SizeFactorCompare"]]
ListOAdj[["HistoList"]][[1]]
ListOAdj[["SFHistos"]][[1]]

SizeFactorsOwn <- ListOAdj[["SFs"]]
```


## Compare to DESeq normalisation pipeline



```{r, warning = FALSE, message = FALSE}

## do their normalisation
total = median(sample_sums(GPhf))
standf = function(x, t=total) round(t * (x / sum(x)))
GPhfs = transform_sample_counts(GPhf, standf)
#range(sample_sums(GPhfs))

## change to relative abundance here
GPhfs = transform_sample_counts(GPhfs, function(x){x/sum(x)})

## keep only taxa with good variation
GPhfsf = filter_taxa(GPhfs, function(x) sd(x)/mean(x) > 3.0, prune = TRUE) # down to 388 taxa
# sample_sums(GPhfsf) # again very different but I think fine since all have now same taxa

# ------------- Calculate distances between samples with different measures (phyloseq::distance) -------
dist_methods <- unlist(distanceMethodList)
# I reduce to
dist_methods <- dist_methods[dist_methods %in% c("unifrac", "wunifrac", "jsd", "bray", "jaccard", "euclidean")]

Distlist <- vector("list", length(dist_methods))
names(Distlist) = dist_methods

for( i in dist_methods ){
        # Calculate distance matrix
        iDist <- phyloseq::distance(GPhfsf, method=i)
        Distlist[[i]] = iDist
}


# ========= recapitulate Bray Crutis Distance ==========
# physeq <- GPhfsf
# if(taxa_are_rows(physeq)){
#         DF <- as(otu_table(physeq), "matrix")
# } else {
#         DF <- as(t(otu_table(physeq)), "matrix")
# }
# 
# ## Long way (computationally) with apply and sapply:
# BrayDistMatrix <- sapply(1:ncol(DF), function(y){apply(DF, 2, function(x){ sum(abs(x-DF[,y]))/sum(x + DF[,y]) })})
# colnames(BrayDistMatrix) <- rownames(BrayDistMatrix)
# # compare:
# all.equal(as(Distlist[["bray"]], "matrix"), BrayDistMatrix, check.attributes = FALSE) # TRUE

# --------------------- Ordination Plots: PCoA and NMDS ----------------------

PCoAList <- vector("list", length(dist_methods))
PCoAPlotList <- vector("list", length(dist_methods))
NMDSList <- vector("list", length(dist_methods))
NMDSPlotList <- vector("list", length(dist_methods))

for (i in seq_along(dist_methods)) {
        
        PCoAList[[i]] <- ordinate(GPhfsf, "PCoA", distance=Distlist[[i]])
        #NMDSList[[i]] <- ordinate(GPhfsf, "NMDS", distance=Distlist[[i]])
        PCoAPlotList[[i]] <- plot_ordination(GPhfsf, PCoAList[[i]], color="SampleType") + ggtitle(names(Distlist)[i])
        #NMDSPlotList[[i]] <- plot_ordination(GPhfsf, NMDSList[[i]], color="SampleType") + ggtitle(names(Distlist)[i])
}
names(PCoAList) <- names(NMDSList) <- names(PCoAPlotList) <- names(NMDSPlotList) <- names(Distlist)

do.call("grid.arrange", c(PCoAPlotList, ncol = 2))

# do.call("grid.arrange", c(NMDSPlotList, ncol = 2))

```



